{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installing the required packages for the project\n",
        "\n",
        "Transformers -> To generate question and answers from the pretrained model\n",
        "\n",
        "Pypdf-> To extract the text content from PDF\n",
        "PDF Extraction\n",
        "Extracting the content of the PDF files to text using pypdf library\n",
        "\n",
        "PYPDF\n",
        "\n",
        "It is one of the built-in python library to work with PDF files such as Extracting text, Merging, Editing etc...\n",
        "\n",
        "Version: 3.16.4\n",
        "\n",
        "Only extracting the Text hence importing PDFReader. File handling method is used to open the pdf, rb- as the file have images etc. To check the total num of pages For loop is used to read the content of all the pages in the pdf\n",
        "\n",
        "Steps in Generating Q&A\n",
        "\n",
        "1. Analyzing the content\n",
        "2. Identifying Keywords\n",
        "3. Transformers\n",
        "4. Q&A generation\n",
        "\n",
        "T5 model- Text To Text Transfer Transformer \"t5-small\"\n"
      ],
      "metadata": {
        "id": "0U6GiSero6HV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install pypdf\n",
        "\n"
      ],
      "metadata": {
        "id": "Wkk0AAph8__P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "470d926a-f3ba-4a64-cac5-4041829b32a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers[sentencepiece]\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers[sentencepiece])\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers[sentencepiece])\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers[sentencepiece])\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.66.1)\n",
            "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece])\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[sentencepiece]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[sentencepiece]) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers[sentencepiece])\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2023.7.22)\n",
            "Installing collected packages: sentencepiece, safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 sentencepiece-0.1.99 tokenizers-0.14.1 transformers-4.34.1\n",
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (41.0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
            "Installing collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20221105\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-3.16.4-py3-none-any.whl (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.6/276.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.16.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cloning the github repo to access the pdf file"
      ],
      "metadata": {
        "id": "VnCKspKGyDFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/akaiketech/internship-assignment-nlp"
      ],
      "metadata": {
        "id": "t1ewx5gJyBPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e66158ea-d04c-49fc-c32b-bffbd6af5481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'internship-assignment-nlp'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 18 (delta 5), reused 5 (delta 2), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (18/18), 8.94 MiB | 13.67 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hK1zVBIhpGjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the req pre trained model from transformer along with the Pdf extracter\n",
        "\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from pypdf import PdfReader\n",
        "\n",
        "def text_extract(pdf_file):\n",
        "    # this will check whether the file provided is in string format or not,If not raises an error.\n",
        "\n",
        "    if not isinstance(pdf_file, str):\n",
        "        raise ValueError(\"Invalid argument type. 'pdf_file' should be a string.\")\n",
        "\n",
        "    with open(pdf_file, 'rb') as pdf:\n",
        "        read_pdf = PdfReader(pdf)\n",
        "        page_nums = len(read_pdf.pages)\n",
        "        text = ''\n",
        "        for pages in read_pdf.pages:\n",
        "            text += pages.extract_text()\n",
        "    return text\n",
        "\n",
        "# Load T5 model and tokenizer\n",
        "model_name = \"t5-small\"\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "def generate_mcma_question(pdf_file):\n",
        "    text_content = text_extract(pdf_file)\n",
        "\n",
        "    # Setting up the input string with placeholders(here we are using 4 placeholders)\n",
        "    # content- the extratcted text will be passed in to this as a passage to generate Question,Answer,and Options\n",
        "\n",
        "    input_string = \"Question: [Question] ; Answer: [Answer] ; Options: [Options] ;  Content: [Content]\"\n",
        "    input_text = input_string.replace(\"[Content]\", text_content)\n",
        "    #print(input_text)\n",
        "\n",
        "\n",
        "    # Tokenize and encode the input text, Encoded input will be stored in input_ids\n",
        "    # have set the maximum length, the input will be taken till the set max_length\n",
        "    # inputs will be truncated once the set max length is exceeded\n",
        "    # By setting return_tensors=\"pt\", the tokenizer will return the encoded inputs as PyTorch tensors\n",
        "\n",
        "    inputs = tokenizer.encode_plus(input_text, return_tensors=\"pt\", max_length=5000, truncation=True)\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "\n",
        "\n",
        "    # taking the inputs,output will be generated.\n",
        "    # Num_return_sequences and num_beams - defines the number of outputs required to generate accoridngly (here we will get 20 questions in output) and beam search width\n",
        "    # tensors will be converted to string again ,conversion will happen in batch and skipping the special characters\n",
        "\n",
        "    output = model.generate(input_ids, max_length=100, num_beams=10, num_return_sequences=10)\n",
        "    decoded_outputs = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "    return decoded_outputs\n",
        "\n",
        "\n",
        "pdf_file = \"/content/internship-assignment-nlp/Dataset/chapter-2.pdf\"\n",
        "generated_output = generate_mcma_question(pdf_file)\n",
        "#print(generated_output)\n",
        "#print(text_content)\n",
        "\n",
        "\n",
        "# Iterating the output to split the questions,answers and MCoptions which will be stored in a empty list as tuples by replacing the placeholder text.\n",
        "# Printing the Questions along with its answer and MC options\n",
        "\n",
        "\n",
        "mcma_ques = []\n",
        "\n",
        "for out in generated_output:\n",
        "    components = out.split(\" \")\n",
        "    if len(components) >= 4:\n",
        "        question = components[0].strip().replace(\"Question:\",\"\")\n",
        "        crct_ans = [a.strip() for a in components[1].replace(\"Answer:\",\"\").split(',')]\n",
        "        options = [o.strip() for o in components[2].replace(\"Options:\",\"\").split(',')]\n",
        "\n",
        "        mcma_ques.append((question,crct_ans,options))\n",
        "    else:\n",
        "        print(f\"Invalid format encountered. Components: {components}\")\n",
        "\n",
        "for i, (question,crct_ans,options) in enumerate(mcma_ques, 1):\n",
        "    print(f\" {i}.  Question: {question}\")\n",
        "    for j, option in enumerate(options, 1):\n",
        "        print(f\" Options: {option}\")\n",
        "    print(f\" Answer: {', '.join(crct_ans)}\")\n",
        "    print()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nCvmvkryODyS",
        "outputId": "ef544142-c63e-4bb4-9d9e-a451b410a950",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1.  Question: the\n",
            " Options: had\n",
            " Answer: Company\n",
            "\n",
            " 2.  Question: the\n",
            " Options: had\n",
            " Answer: Company\n",
            "\n",
            " 3.  Question: the\n",
            " Options: had\n",
            " Answer: Company\n",
            "\n",
            " 4.  Question: the\n",
            " Options: had\n",
            " Answer: Company\n",
            "\n",
            " 5.  Question: the\n",
            " Options: had\n",
            " Answer: Company\n",
            "\n",
            " 6.  Question: the\n",
            " Options: had\n",
            " Answer: Company\n",
            "\n",
            " 7.  Question: the\n",
            " Options: had\n",
            " Answer: Company\n",
            "\n",
            " 8.  Question: the\n",
            " Options: had\n",
            " Answer: Company\n",
            "\n",
            " 9.  Question: the\n",
            " Options: had\n",
            " Answer: Company\n",
            "\n",
            " 10.  Question: the\n",
            " Options: had\n",
            " Answer: Company\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vdW94IHlODmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "58NBavgLODTz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}